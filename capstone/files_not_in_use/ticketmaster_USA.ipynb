{
 "cells": [
  {
   "cell_type": "raw",
   "id": "liked-importance",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "outside-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your API key Lqseo1SMwgdKqLEKOPTLlfJaLSCMtd0S\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': [{'code': 'DIS1035', 'detail': 'API Limits Exceeded: Max paging depth exceeded. (page * size) must be less than 1,000', 'status': '400', '_links': {'about': {'href': '/discovery/v2/errors.html#DIS1035'}}}]}\n",
      "successfully pulled 5 USA pages and 800 events\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "api_key = input('Enter your API key')\n",
    "\n",
    "list_of_usa_events = []\n",
    "index = 1\n",
    "responses = []\n",
    "\n",
    "\n",
    "while index<12:\n",
    "\n",
    "    url = f'https://app.ticketmaster.com/discovery/v2/events.json?size=200&page={index}&countryCode=US&apikey={api_key}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    responses.append(response)\n",
    "    data=response.text\n",
    "\n",
    "    parsed = json.loads(data)\n",
    "\n",
    "    # Combine all the events\n",
    "    try:\n",
    "        list_of_usa_events.extend(parsed.get('_embedded').get('events'))\n",
    "        index+=1\n",
    "\n",
    "    except:\n",
    "        index+=1\n",
    "        print(parsed)\n",
    "#         continue\n",
    "        break\n",
    "#     index+=1\n",
    "\n",
    "print(\"successfully pulled\",index-1,\"USA pages and\",len(list_of_usa_events), \"events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-shame",
   "metadata": {},
   "source": [
    "# GET EVENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fifteen-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the events in a dataframe\n",
    "events = pd.DataFrame({'events':list_of_usa_events})\n",
    "\n",
    "# Break the disctionaries into columns\n",
    "# Now we have 1 column for each sales, locale, dates, etc\n",
    "iteration_1 = events['events'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "headed-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the event names and add them to a dataframe\n",
    "list_of_event_names = [event.get('name') for event in list_of_usa_events]\n",
    "list_of_event_ids = [event.get('id') for event in list_of_usa_events]\n",
    "test = [event.get('test') for event in list_of_usa_events]\n",
    "\n",
    "event_names_df = pd.DataFrame({'event name':list_of_event_names, 'event id':list_of_event_ids, 'is this entry a test?':test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-bundle",
   "metadata": {},
   "source": [
    "## VENUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "other-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df = iteration_1._embedded.apply(pd.Series)\n",
    "\n",
    "list_of_venue_dictionaries = [embedded_df.venues[index][0] for index in range(len(embedded_df.venues))] \n",
    "\n",
    "venue_df = pd.DataFrame({'venue_dictionaries':list_of_venue_dictionaries})\n",
    "venue_final = pd.DataFrame(venue_df['venue_dictionaries'].apply(pd.Series)['name'])\n",
    "venue_final.rename({'name': 'venue name'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-struggle",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "minimal-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the sales column into columns\n",
    "# We now have public sales and presales\n",
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "\n",
    "# Break the public sales into columns\n",
    "sales_public = sales.public.apply(pd.Series)\n",
    "\n",
    "# rename the public sales columns to distinguish them from the rest\n",
    "for col in sales_public.columns:\n",
    "    sales_public.rename({col:f'public sales {col}'}, axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dataframe to add the final product of the steps below\n",
    "presale_final = pd.DataFrame()\n",
    "\n",
    "# Looping through the presales column to extract the information from the dictionaries\n",
    "# each presales row will have a dictionary with all the presale methods \n",
    "for presales_list_element in sales.presales:\n",
    "    \n",
    "# \"\"\"\n",
    "# If there is no presale, the entire rowcell will be NaN\n",
    "# In this case the pandas.isnull method will give us an error\n",
    "# We will need to add a new blank row in the except statement\n",
    "# We will also add a new column called 'blank' to make sure we don't mess the other columns names\n",
    "# The column will be dropped at the end\n",
    "# \"\"\"\n",
    "    try:\n",
    "        length = len(pd.isnull(presales_list_element))\n",
    "    except:\n",
    "        dict_holder_df = pd.DataFrame({'blank':[np.nan]})\n",
    "        presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "        continue\n",
    "\n",
    "# \"\"\"\n",
    "# The dictionaries will contain the name of the presale as well as the end and start date. \n",
    "# We need to extract the name and move it to the column names so that we can distinguish the start and end dates of each presale\n",
    "# \"\"\"\n",
    "    dict_holder_df = pd.DataFrame()\n",
    "    for presale_dict in presales_list_element:\n",
    "        \n",
    "        start_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[0]\n",
    "        end_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[1]\n",
    "        \n",
    "        holder_df = pd.DataFrame({start_time_title:[presale_dict['startDateTime']], end_time_title:[presale_dict['endDateTime']]})\n",
    "        dict_holder_df = pd.concat([dict_holder_df,holder_df], axis=1)\n",
    "        \n",
    "    presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "\n",
    "\n",
    "# Reset the index so that we can merge\n",
    "# drop the blank column\n",
    "presale_final.reset_index(inplace=True)\n",
    "presale_final.drop(columns=['index', 'blank'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-unemployment",
   "metadata": {},
   "source": [
    "# DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "desperate-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the dates column into columns\n",
    "# There are a lot of dates trapped into disctionaries. Use pd.Series multiple times to untangle everything\n",
    "dates = iteration_1.dates.apply(pd.Series)\n",
    "start_dates = dates.start.apply(pd.Series)\n",
    "status = dates.status.apply(pd.Series)\n",
    "initial_start_date = dates.initialStartDate.apply(pd.Series)\n",
    "\n",
    "# Make sure there are no weird columns created by pd.series\n",
    "initial_start_date = initial_start_date[['dateTime', 'localDate', 'localTime']]\n",
    "\n",
    "# rename the dates columns to distinguish them\n",
    "for col in start_dates.columns:\n",
    "    start_dates.rename({col:f'event start {col}'}, axis = 1, inplace = True)\n",
    "\n",
    "for col in initial_start_date.columns:\n",
    "    initial_start_date.rename({col:f'event initial start {col}'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-activity",
   "metadata": {},
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "laughing-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "list_of_classification_dictionaries = [iteration_1.classifications[index][0] for index in range(len(iteration_1.classifications))] \n",
    "\n",
    "classification_df = pd.DataFrame({'classification_dictionaries':list_of_classification_dictionaries})\n",
    "classification_df_broken_in_columns = classification_df['classification_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "classification_df_primary_family = classification_df_broken_in_columns[['primary', 'family']]\n",
    "columns_of_interest = list(classification_df_broken_in_columns.columns)\n",
    "columns_of_interest.remove('primary')\n",
    "columns_of_interest.remove('family')\n",
    "\n",
    "classification_df_final = pd.DataFrame()\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    series_holder = classification_df_broken_in_columns[column].apply(pd.Series)\n",
    "    series_holder.drop(columns = ['id'], inplace=True)\n",
    "    series_holder.rename({series_holder.columns[0]:column}, inplace = True, axis = 1)\n",
    "    classification_df_final = pd.concat([classification_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-birmingham",
   "metadata": {},
   "source": [
    "# PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "auburn-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the promoters column into columns\n",
    "promoters = iteration_1.promoters.apply(pd.Series)\n",
    "\n",
    "for index in range(len(promoters.columns)):\n",
    "    promoters.rename({promoters.columns[index]: 'event promoter' + ' ' + str(promoters.columns[index]+1)}, axis = 1, inplace = True)\n",
    "\n",
    "promoters_df_final = pd.DataFrame()\n",
    "\n",
    "for column in promoters.columns:\n",
    "    series_holder = promoters[column].apply(pd.Series)\n",
    "    series_holder = series_holder[['name', 'description']]\n",
    "    series_holder.rename({'name':column + ' ' + 'name', 'description':column + ' ' + 'description'}, inplace = True, axis = 1)\n",
    "    promoters_df_final = pd.concat([promoters_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-choice",
   "metadata": {},
   "source": [
    "# PRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "intelligent-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the price information is missing from an event. \n",
    "# Replace NA with the word missing\n",
    "iteration_1.fillna({'priceRanges':'missing'}, inplace=True)\n",
    "\n",
    "\n",
    "# create a genberic pricing structure to replace the missing pricing information\n",
    "# set all prices to zero\n",
    "generic_pricing_structure = [{'type': 'standard', 'currency': 'USD', 'min': -1, 'max': -1}]        \n",
    "\n",
    "# Unfortunately the replace will get rid of the list part and replace everything with a 'naked' dictionary\n",
    "# Ideally we would have a list with the generic pricing dictionary as its only element\n",
    "test = iteration_1.replace({'missing':generic_pricing_structure})\n",
    "\n",
    "\n",
    "# The try-except step will fix this issue. \n",
    "# If we have a list with a dictionary inside, we will fetch the dictionary from inside the list\n",
    "# Otherwise we will fetch the dictionary directly \n",
    "list_of_pricerange_dictionaries = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    try:\n",
    "        x = row['priceRanges'].get('type')\n",
    "        list_of_pricerange_dictionaries.append(row['priceRanges'])\n",
    "    except:\n",
    "        list_of_pricerange_dictionaries.append(row['priceRanges'][0])\n",
    "\n",
    "        \n",
    "pricerange_df = pd.DataFrame({'pricerange_dictionaries':list_of_pricerange_dictionaries})\n",
    "pricerange_df_broken_in_columns = pricerange_df['pricerange_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "pricerange_df_broken_in_columns = pricerange_df_broken_in_columns[['type', 'min', 'max']]\n",
    "pricerange_df_broken_in_columns.rename({'type':'price type', 'min':'price min', 'max':'price max'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-browser",
   "metadata": {},
   "source": [
    "# TICKET LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "instant-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1.columns\n",
    "ticket_limit = pd.DataFrame(iteration_1['ticketLimit'].apply(pd.Series))\n",
    "ticket_limit = pd.DataFrame(ticket_limit['info']) \n",
    "ticket_limit.rename({'info':'ticket limit'},axis = 1, inplace = True)\n",
    "please_note = pd.DataFrame(iteration_1['pleaseNote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-department",
   "metadata": {},
   "source": [
    "# MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "internal-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except run\n"
     ]
    }
   ],
   "source": [
    "new_dataset = pd.concat([\n",
    "    event_names_df\n",
    "    ,venue_final\n",
    "    ,iteration_1['info']\n",
    "    ,sales_public\n",
    "    ,presale_final\n",
    "    ,start_dates\n",
    "    ,status\n",
    "    ,initial_start_date\n",
    "    ,classification_df_final\n",
    "    ,promoters_df_final\n",
    "    ,pricerange_df_broken_in_columns\n",
    "    ,ticket_limit\n",
    "    ,please_note\n",
    "    \n",
    "], axis = 1)\n",
    "\n",
    "list_of_groupby_columns = list(new_dataset.columns)\n",
    "\n",
    "datatypes_df = pd.DataFrame(new_dataset.dtypes, columns = ['data type']).reset_index()\n",
    "\n",
    "list_of_string_variables = list(datatypes_df[datatypes_df['data type'] == 'object']['index'])\n",
    "list_of_boolean_variables = list(datatypes_df[datatypes_df['data type'] == 'bool']['index'])\n",
    "list_of_numeric_variables = list(datatypes_df[datatypes_df['data type'] == 'float']['index'])\n",
    "\n",
    "\n",
    "for var in list_of_groupby_columns:\n",
    "    if var in list_of_string_variables:\n",
    "        new_dataset.fillna({var: '-1'}, inplace = True)\n",
    "    else:\n",
    "        new_dataset.fillna({var: -1}, inplace = True)\n",
    "        \n",
    "\n",
    "today_timestamp = pd.to_datetime(\"today\")\n",
    "today_date = today_timestamp.date()\n",
    "new_dataset['timestamp of data pull'] = today_timestamp\n",
    "new_dataset['date of data pull'] = today_date\n",
    "\n",
    "try:\n",
    "    current_dataset = pd.read_csv('final_usa_ticketmaster_dataset.csv')\n",
    "    appended_dataset = pd.concat([current_dataset, new_dataset], axis = 0)\n",
    "    appended_dataset.drop(columns = [appended_dataset.columns[0]], inplace = True)\n",
    "\n",
    "    appended_dataset['timestamp of data pull'] = pd.to_datetime(appended_dataset['timestamp of data pull'])\n",
    "    appended_dataset['date of data pull'] = pd.to_datetime(appended_dataset['date of data pull'])\n",
    "\n",
    "    updated_dataset = appended_dataset.groupby(list_of_groupby_columns, as_index = False) \\\n",
    "    .agg({'date of data pull':'min', 'timestamp of data pull':'min'}) \n",
    "\n",
    "    updated_dataset.to_csv('final_usa_ticketmaster_dataset.csv')\n",
    "\n",
    "    extra_events = updated_dataset['event id'].nunique() - current_dataset['event id'].nunique()  \n",
    "    extra_rows = updated_dataset.shape[0] - current_dataset.shape[0]  \n",
    "    print ('This pull resulted in', extra_rows, 'new rows')\n",
    "    print ('This pull resulted in', extra_events, 'new events')\n",
    "\n",
    "\n",
    "except:\n",
    "    new_dataset.to_csv('final_usa_ticketmaster_dataset.csv')\n",
    "    print('except run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "opening-tulsa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-malpractice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
