{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "api_key = input('Enter your API key')\n",
    "\n",
    "# pick up Carolina Theater separately, because for some reason it is not part of the Triangle locale. \n",
    "url = f'https://app.ticketmaster.com/discovery/v2/events.json?size=200&venueId=KovZpZAFAl6A&apikey={api_key}'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "data=response.text\n",
    "\n",
    "parsed = json.loads(data)\n",
    "\n",
    "list_of_carolina_theater_events = parsed.get('_embedded').get('events')\n",
    "list_of_triangle_events = []\n",
    "index = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    url = f'https://app.ticketmaster.com/discovery/v2/events.json?page={index}&dmaId=366&size=200&apikey={api_key}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    data=response.text\n",
    "\n",
    "    parsed = json.loads(data)\n",
    "\n",
    "    # Combine all the events\n",
    "    try:\n",
    "        list_of_triangle_events.extend(parsed.get('_embedded').get('events'))\n",
    "        index+=1\n",
    "    except:\n",
    "        print(\"successfully pulled\",index-1,\"Triangle pages and\",len(list_of_triangle_events), \"events\")\n",
    "        break\n",
    "\n",
    "list_of_triangle_events.extend(list_of_carolina_theater_events)\n",
    "\n",
    "print(\"We added\", len(list_of_carolina_theater_events), \"Carolina Theater events for a total of\", len(list_of_triangle_events), \"Triangle events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-scale",
   "metadata": {},
   "source": [
    "# GET EVENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faced-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the events in a dataframe\n",
    "events = pd.DataFrame({'events':list_of_triangle_events})\n",
    "\n",
    "# Break the disctionaries into columns\n",
    "# Now we have 1 column for each sales, locale, dates, etc\n",
    "iteration_1 = events['events'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the event names and add them to a dataframe\n",
    "list_of_event_names = [event.get('name') for event in list_of_triangle_events]\n",
    "list_of_event_ids = [event.get('id') for event in list_of_triangle_events]\n",
    "test = [event.get('test') for event in list_of_triangle_events]\n",
    "\n",
    "event_names_df = pd.DataFrame({'event name':list_of_event_names, 'event id':list_of_event_ids, 'is this entry a test?':test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-surrey",
   "metadata": {},
   "source": [
    "## VENUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minute-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df = iteration_1._embedded.apply(pd.Series)\n",
    "\n",
    "list_of_venue_dictionaries = [embedded_df.venues[index][0] for index in range(len(embedded_df.venues))] \n",
    "\n",
    "venue_df = pd.DataFrame({'venue_dictionaries':list_of_venue_dictionaries})\n",
    "venue_final = pd.DataFrame(venue_df['venue_dictionaries'].apply(pd.Series)['name'])\n",
    "venue_final.rename({'name': 'venue name'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-cleveland",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the sales column into columns\n",
    "# We now have public sales and presales\n",
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "\n",
    "# Break the public sales into columns\n",
    "sales_public = sales.public.apply(pd.Series)\n",
    "\n",
    "# rename the public sales columns to distinguish them from the rest\n",
    "for col in sales_public.columns:\n",
    "    sales_public.rename({col:f'public sales {col}'}, axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dataframe to add the final product of the steps below\n",
    "presale_final = pd.DataFrame()\n",
    "\n",
    "# Looping through the presales column to extract the information from the dictionaries\n",
    "# each presales row will have a dictionary with all the presale methods \n",
    "for presales_list_element in sales.presales:\n",
    "    \n",
    "# \"\"\"\n",
    "# If there is no presale, the entire rowcell will be NaN\n",
    "# In this case the pandas.isnull method will give us an error\n",
    "# We will need to add a new blank row in the except statement\n",
    "# We will also add a new column called 'blank' to make sure we don't mess the other columns names\n",
    "# The column will be dropped at the end\n",
    "# \"\"\"\n",
    "    try:\n",
    "        length = len(pd.isnull(presales_list_element))\n",
    "    except:\n",
    "        dict_holder_df = pd.DataFrame({'blank':[np.nan]})\n",
    "        presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "        continue\n",
    "\n",
    "# \"\"\"\n",
    "# The dictionaries will contain the name of the presale as well as the end and start date. \n",
    "# We need to extract the name and move it to the column names so that we can distinguish the start and end dates of each presale\n",
    "# \"\"\"\n",
    "    dict_holder_df = pd.DataFrame()\n",
    "    for presale_dict in presales_list_element:\n",
    "        \n",
    "        start_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[0]\n",
    "        end_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[1]\n",
    "        \n",
    "        holder_df = pd.DataFrame({start_time_title:[presale_dict['startDateTime']], end_time_title:[presale_dict['endDateTime']]})\n",
    "        dict_holder_df = pd.concat([dict_holder_df,holder_df], axis=1)\n",
    "        \n",
    "    presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "\n",
    "\n",
    "# Reset the index so that we can merge\n",
    "# drop the blank column\n",
    "presale_final.reset_index(inplace=True)\n",
    "presale_final.drop(columns=['index', 'blank'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-pointer",
   "metadata": {},
   "source": [
    "# DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "international-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the dates column into columns\n",
    "# There are a lot of dates trapped into disctionaries. Use pd.Series multiple times to untangle everything\n",
    "dates = iteration_1.dates.apply(pd.Series)\n",
    "start_dates = dates.start.apply(pd.Series)\n",
    "status = dates.status.apply(pd.Series)\n",
    "initial_start_date = dates.initialStartDate.apply(pd.Series)\n",
    "\n",
    "# Make sure there are no weird columns created by pd.series\n",
    "initial_start_date = initial_start_date[['dateTime', 'localDate', 'localTime']]\n",
    "\n",
    "# rename the dates columns to distinguish them\n",
    "for col in start_dates.columns:\n",
    "    start_dates.rename({col:f'event start {col}'}, axis = 1, inplace = True)\n",
    "\n",
    "for col in initial_start_date.columns:\n",
    "    initial_start_date.rename({col:f'event initial start {col}'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-soundtrack",
   "metadata": {},
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indonesian-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "list_of_classification_dictionaries = [iteration_1.classifications[index][0] for index in range(len(iteration_1.classifications))] \n",
    "\n",
    "classification_df = pd.DataFrame({'classification_dictionaries':list_of_classification_dictionaries})\n",
    "classification_df_broken_in_columns = classification_df['classification_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "classification_df_primary_family = classification_df_broken_in_columns[['primary', 'family']]\n",
    "columns_of_interest = list(classification_df_broken_in_columns.columns)\n",
    "columns_of_interest.remove('primary')\n",
    "columns_of_interest.remove('family')\n",
    "\n",
    "classification_df_final = pd.DataFrame()\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    series_holder = classification_df_broken_in_columns[column].apply(pd.Series)\n",
    "    series_holder.drop(columns = ['id'], inplace=True)\n",
    "    series_holder.rename({series_holder.columns[0]:column}, inplace = True, axis = 1)\n",
    "    classification_df_final = pd.concat([classification_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-registration",
   "metadata": {},
   "source": [
    "# PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imported-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the promoters column into columns\n",
    "promoters = iteration_1.promoters.apply(pd.Series)\n",
    "\n",
    "for index in range(len(promoters.columns)):\n",
    "    promoters.rename({promoters.columns[index]: 'event promoter' + ' ' + str(promoters.columns[index]+1)}, axis = 1, inplace = True)\n",
    "\n",
    "promoters_df_final = pd.DataFrame()\n",
    "\n",
    "for column in promoters.columns:\n",
    "    series_holder = promoters[column].apply(pd.Series)\n",
    "    series_holder = series_holder[['name', 'description']]\n",
    "    series_holder.rename({'name':column + ' ' + 'name', 'description':column + ' ' + 'description'}, inplace = True, axis = 1)\n",
    "    promoters_df_final = pd.concat([promoters_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-mainland",
   "metadata": {},
   "source": [
    "# PRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "small-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the price information is missing from an event. \n",
    "# Replace NA with the word missing\n",
    "iteration_1.fillna({'priceRanges':'missing'}, inplace=True)\n",
    "\n",
    "\n",
    "# create a genberic pricing structure to replace the missing pricing information\n",
    "# set all prices to zero\n",
    "generic_pricing_structure = [{'type': 'standard', 'currency': 'USD', 'min': -1, 'max': -1}]        \n",
    "\n",
    "# Unfortunately the replace will get rid of the list part and replace everything with a 'naked' dictionary\n",
    "# Ideally we would have a list with the generic pricing dictionary as its only element\n",
    "test = iteration_1.replace({'missing':generic_pricing_structure})\n",
    "\n",
    "\n",
    "# The try-except step will fix this issue. \n",
    "# If we have a list with a dictionary inside, we will fetch the dictionary from inside the list\n",
    "# Otherwise we will fetch the dictionary directly \n",
    "list_of_pricerange_dictionaries = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    try:\n",
    "        x = row['priceRanges'].get('type')\n",
    "        list_of_pricerange_dictionaries.append(row['priceRanges'])\n",
    "    except:\n",
    "        list_of_pricerange_dictionaries.append(row['priceRanges'][0])\n",
    "\n",
    "        \n",
    "pricerange_df = pd.DataFrame({'pricerange_dictionaries':list_of_pricerange_dictionaries})\n",
    "pricerange_df_broken_in_columns = pricerange_df['pricerange_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "pricerange_df_broken_in_columns = pricerange_df_broken_in_columns[['type', 'min', 'max']]\n",
    "pricerange_df_broken_in_columns.rename({'type':'price type', 'min':'price min', 'max':'price max'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-reception",
   "metadata": {},
   "source": [
    "# TICKET LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "communist-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1.columns\n",
    "ticket_limit = pd.DataFrame(iteration_1['ticketLimit'].apply(pd.Series))\n",
    "ticket_limit = pd.DataFrame(ticket_limit['info']) \n",
    "ticket_limit.rename({'info':'ticket limit'},axis = 1, inplace = True)\n",
    "please_note = pd.DataFrame(iteration_1['pleaseNote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-aviation",
   "metadata": {},
   "source": [
    "# MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intimate-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pull resulted in 1 new rows\n",
      "This pull resulted in 0 new events\n"
     ]
    }
   ],
   "source": [
    "new_dataset = pd.concat([\n",
    "    event_names_df\n",
    "    ,venue_final\n",
    "    ,iteration_1['info']\n",
    "    ,sales_public\n",
    "    ,presale_final\n",
    "    ,start_dates\n",
    "    ,status\n",
    "    ,initial_start_date\n",
    "    ,classification_df_final\n",
    "    ,promoters_df_final\n",
    "    ,pricerange_df_broken_in_columns\n",
    "    ,ticket_limit\n",
    "    ,please_note\n",
    "    \n",
    "], axis = 1)\n",
    "\n",
    "list_of_groupby_columns = list(new_dataset.columns)\n",
    "\n",
    "datatypes_df = pd.DataFrame(new_dataset.dtypes, columns = ['data type']).reset_index()\n",
    "\n",
    "list_of_string_variables = list(datatypes_df[datatypes_df['data type'] == 'object']['index'])\n",
    "list_of_boolean_variables = list(datatypes_df[datatypes_df['data type'] == 'bool']['index'])\n",
    "list_of_numeric_variables = list(datatypes_df[datatypes_df['data type'] == 'float']['index'])\n",
    "\n",
    "\n",
    "for var in list_of_groupby_columns:\n",
    "    if var in list_of_string_variables:\n",
    "        new_dataset.fillna({var: '-1'}, inplace = True)\n",
    "    else:\n",
    "        new_dataset.fillna({var: -1}, inplace = True)\n",
    "        \n",
    "\n",
    "today_timestamp = pd.to_datetime(\"today\")\n",
    "today_date = today_timestamp.date()\n",
    "new_dataset['timestamp of data pull'] = today_timestamp\n",
    "new_dataset['date of data pull'] = today_date\n",
    "\n",
    "try:\n",
    "    current_dataset = pd.read_csv('final_ticketmaster_dataset.csv')\n",
    "    appended_dataset = pd.concat([current_dataset, new_dataset], axis = 0)\n",
    "    appended_dataset.drop(columns = [appended_dataset.columns[0]], inplace = True)\n",
    "\n",
    "    appended_dataset['timestamp of data pull'] = pd.to_datetime(appended_dataset['timestamp of data pull'])\n",
    "    appended_dataset['date of data pull'] = pd.to_datetime(appended_dataset['date of data pull'])\n",
    "\n",
    "    updated_dataset = appended_dataset.groupby(list_of_groupby_columns, as_index = False) \\\n",
    "    .agg({'date of data pull':'min', 'timestamp of data pull':'min'}) \n",
    "\n",
    "    updated_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "\n",
    "    extra_events = updated_dataset['event id'].nunique() - current_dataset['event id'].nunique()  \n",
    "    extra_rows = updated_dataset.shape[0] - current_dataset.shape[0]  \n",
    "    print ('This pull resulted in', extra_rows, 'new rows')\n",
    "    print ('This pull resulted in', extra_events, 'new events')\n",
    "\n",
    "\n",
    "except:\n",
    "    new_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "    print('except run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graphic-twist",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\\\third_party_sales_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1a35478356c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~\\third_party_sales_1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\\\third_party_sales_1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('~\\third_party_sales_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-ultimate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
