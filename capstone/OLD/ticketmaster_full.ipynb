{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cooperative-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your API key Lqseo1SMwgdKqLEKOPTLlfJaLSCMtd0S\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "api_key = input('Enter your API key')\n",
    "\n",
    "# TRIANGLE\n",
    "# url = f'https://app.ticketmaster.com/discovery/v2/events.json?dmaId=366&&size=50&apikey={api_key}'\n",
    "\n",
    "\n",
    "# ENTIRE US\n",
    "url = f'https://app.ticketmaster.com/discovery/v2/events.json?size=200&page=1&countryCode=US&apikey={api_key}'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "data=response.text\n",
    "\n",
    "parsed_us1 = json.loads(data)\n",
    "\n",
    "url = f'https://app.ticketmaster.com/discovery/v2/events.json?size=200&page=2&countryCode=US&apikey={api_key}'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "data=response.text\n",
    "\n",
    "parsed_us2 = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-monaco",
   "metadata": {},
   "source": [
    "# GET EVENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "advised-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the events\n",
    "list_of_events = parsed_us1.get('_embedded').get('events')\n",
    "list_of_events.extend(parsed_us2.get('_embedded').get('events'))\n",
    "\n",
    "# Put all the events in a dataframe\n",
    "events = pd.DataFrame({'events':list_of_events})\n",
    "\n",
    "# Break the disctionaries into columns\n",
    "# Now we have 1 column for each sales, locale, dates, etc\n",
    "iteration_1 = events['events'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "listed-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the event names and add them to a dataframe\n",
    "list_of_event_names = [event.get('name') for event in list_of_events]\n",
    "list_of_event_ids = [event.get('id') for event in list_of_events]\n",
    "test = [event.get('test') for event in list_of_events]\n",
    "\n",
    "event_names_df = pd.DataFrame({'event name':list_of_event_names, 'event id':list_of_event_ids, 'is this entry a test?':test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-castle",
   "metadata": {},
   "source": [
    "## VENUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cognitive-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df = iteration_1._embedded.apply(pd.Series)\n",
    "\n",
    "list_of_venue_dictionaries = [embedded_df.venues[index][0] for index in range(len(embedded_df.venues))] \n",
    "\n",
    "venue_df = pd.DataFrame({'venue_dictionaries':list_of_venue_dictionaries})\n",
    "venue_final = pd.DataFrame(venue_df['venue_dictionaries'].apply(pd.Series)['name'])\n",
    "venue_final.rename({'name': 'venue name'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-dakota",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "heavy-commons",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'presales'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-3ddb26511d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Looping through the presales column to extract the information from the dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# each presales row will have a dictionary with all the presale methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpresales_list_element\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresales\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioannisloukakis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'presales'"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the sales column into columns\n",
    "# We now have public sales and presales\n",
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "\n",
    "# Break the public sales into columns\n",
    "sales_public = sales.public.apply(pd.Series)\n",
    "\n",
    "# rename the public sales columns to distinguish them from the rest\n",
    "for col in sales_public.columns:\n",
    "    sales_public.rename({col:f'public sales {col}'}, axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dataframe to add the final product of the steps below\n",
    "presale_final = pd.DataFrame()\n",
    "\n",
    "# Looping through the presales column to extract the information from the dictionaries\n",
    "# each presales row will have a dictionary with all the presale methods \n",
    "for presales_list_element in sales.presales:\n",
    "    \n",
    "# \"\"\"\n",
    "# If there is no presale, the entire rowcell will be NaN\n",
    "# In this case the pandas.isnull method will give us an error\n",
    "# We will need to add a new blank row in the except statement\n",
    "# We will also add a new column called 'blank' to make sure we don't mess the other columns names\n",
    "# The column will be dropped at the end\n",
    "# \"\"\"\n",
    "    try:\n",
    "        length = len(pd.isnull(presales_list_element))\n",
    "    except:\n",
    "        dict_holder_df = pd.DataFrame({'blank':[np.nan]})\n",
    "        presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "        continue\n",
    "\n",
    "# \"\"\"\n",
    "# The dictionaries will contain the name of the presale as well as the end and start date. \n",
    "# We need to extract the name and move it to the column names so that we can distinguish the start and end dates of each presale\n",
    "# \"\"\"\n",
    "    dict_holder_df = pd.DataFrame()\n",
    "    for presale_dict in presales_list_element:\n",
    "        \n",
    "        start_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[0]\n",
    "        end_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[1]\n",
    "        \n",
    "        holder_df = pd.DataFrame({start_time_title:[presale_dict['startDateTime']], end_time_title:[presale_dict['endDateTime']]})\n",
    "        dict_holder_df = pd.concat([dict_holder_df,holder_df], axis=1)\n",
    "        \n",
    "    presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "\n",
    "\n",
    "# Reset the index so that we can merge\n",
    "# drop the blank column\n",
    "presale_final.reset_index(inplace=True)\n",
    "presale_final.drop(columns=['index', 'blank'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "vocational-publicity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['public'], dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "sales.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-contributor",
   "metadata": {},
   "source": [
    "# DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sublime-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the dates column into columns\n",
    "# There are a lot of dates trapped into disctionaries. Use pd.Series multiple times to untangle everything\n",
    "dates = iteration_1.dates.apply(pd.Series)\n",
    "start_dates = dates.start.apply(pd.Series)\n",
    "status = dates.status.apply(pd.Series)\n",
    "initial_start_date = dates.initialStartDate.apply(pd.Series)\n",
    "\n",
    "# Make sure there are no weird columns created by pd.series\n",
    "initial_start_date = initial_start_date[['dateTime', 'localDate', 'localTime']]\n",
    "\n",
    "# rename the dates columns to distinguish them\n",
    "for col in start_dates.columns:\n",
    "    start_dates.rename({col:f'event start {col}'}, axis = 1, inplace = True)\n",
    "\n",
    "for col in initial_start_date.columns:\n",
    "    initial_start_date.rename({col:f'event initial start {col}'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-circle",
   "metadata": {},
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continuous-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "list_of_classification_dictionaries = [iteration_1.classifications[index][0] for index in range(len(iteration_1.classifications))] \n",
    "\n",
    "classification_df = pd.DataFrame({'classification_dictionaries':list_of_classification_dictionaries})\n",
    "classification_df_broken_in_columns = classification_df['classification_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "classification_df_primary_family = classification_df_broken_in_columns[['primary', 'family']]\n",
    "columns_of_interest = list(classification_df_broken_in_columns.columns)\n",
    "columns_of_interest.remove('primary')\n",
    "columns_of_interest.remove('family')\n",
    "\n",
    "classification_df_final = pd.DataFrame()\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    series_holder = classification_df_broken_in_columns[column].apply(pd.Series)\n",
    "    series_holder.drop(columns = ['id'], inplace=True)\n",
    "    series_holder.rename({series_holder.columns[0]:column}, inplace = True, axis = 1)\n",
    "    classification_df_final = pd.concat([classification_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-bailey",
   "metadata": {},
   "source": [
    "# PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "simple-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the promoters column into columns\n",
    "promoters = iteration_1.promoters.apply(pd.Series)\n",
    "\n",
    "for index in range(len(promoters.columns)):\n",
    "    promoters.rename({promoters.columns[index]: 'event promoter' + ' ' + str(promoters.columns[index]+1)}, axis = 1, inplace = True)\n",
    "\n",
    "promoters_df_final = pd.DataFrame()\n",
    "\n",
    "for column in promoters.columns:\n",
    "    series_holder = promoters[column].apply(pd.Series)\n",
    "    series_holder = series_holder[['name', 'description']]\n",
    "    series_holder.rename({'name':column + ' ' + 'name', 'description':column + ' ' + 'description'}, inplace = True, axis = 1)\n",
    "    promoters_df_final = pd.concat([promoters_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-intensity",
   "metadata": {},
   "source": [
    "# PRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerous-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pricerange_dictionaries = [iteration_1.priceRanges[index][0] for index in range(len(iteration_1.priceRanges))] \n",
    "\n",
    "pricerange_df = pd.DataFrame({'pricerange_dictionaries':list_of_pricerange_dictionaries})\n",
    "pricerange_df_broken_in_columns = pricerange_df['pricerange_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "pricerange_df_broken_in_columns = pricerange_df_broken_in_columns[['type', 'min', 'max']]\n",
    "pricerange_df_broken_in_columns.rename({'type':'price type', 'min':'price min', 'max':'price max'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-defeat",
   "metadata": {},
   "source": [
    "# TICKET LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disabled-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1.columns\n",
    "ticket_limit = pd.DataFrame(iteration_1['ticketLimit'].apply(pd.Series))\n",
    "ticket_limit = pd.DataFrame(ticket_limit['info']) \n",
    "ticket_limit.rename({'info':'ticket limit'},axis = 1, inplace = True)\n",
    "please_note = pd.DataFrame(iteration_1['pleaseNote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-alloy",
   "metadata": {},
   "source": [
    "# MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thermal-alexandria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pull resulted in 1 new events\n"
     ]
    }
   ],
   "source": [
    "new_dataset = pd.concat([\n",
    "    event_names_df\n",
    "    ,venue_final\n",
    "    ,iteration_1['info']\n",
    "    ,sales_public\n",
    "    ,presale_final\n",
    "    ,start_dates\n",
    "    ,status\n",
    "    ,initial_start_date\n",
    "    ,classification_df_final\n",
    "    ,promoters_df_final\n",
    "    ,pricerange_df_broken_in_columns\n",
    "    ,ticket_limit\n",
    "    ,please_note\n",
    "    \n",
    "], axis = 1)\n",
    "\n",
    "\n",
    "try:\n",
    "    current_dataset = pd.read_csv('final_ticketmaster_dataset.csv')\n",
    "    current_ids = list(current_dataset['event id'])\n",
    "\n",
    "    new_ids = list(new_dataset['event id'])\n",
    "\n",
    "    updated_dataset = pd.concat([current_dataset, new_dataset[new_dataset['event id'].isin(np.setdiff1d(new_ids,current_ids))]], axis = 0)\n",
    "    updated_dataset.drop(columns=[updated_dataset.columns[0]],inplace = True)\n",
    "    updated_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "\n",
    "    extra_ids = list(np.setdiff1d(new_ids,current_ids))\n",
    "    print ('This pull resulted in', len(extra_ids), 'new events')\n",
    "\n",
    "\n",
    "except:\n",
    "    new_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "    print('except run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mexican-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_dataset.columns\n",
    "# new_dataset.reset_index().columns\n",
    "\n",
    "# check = new_dataset.reset_index()\n",
    "# check.drop(columns=['index'], inplace = True)\n",
    "# check.to_csv('final_ticketmaster_dataset.csv')\n",
    "\n",
    "# new_dataset.reset_index().drop(columns=['index'], inplace = True).head()\n",
    "\n",
    "# new_dataset.head()\n",
    "# new_dataset.reset_index().head()\n",
    "\n",
    "# event_names_df.columns\n",
    "# venue_final.columns\n",
    "# sales_public.columns\n",
    "# presale_final.columns\n",
    "# start_dates.columns\n",
    "# status.columns\n",
    "# initial_start_date.columns\n",
    "# classification_df_final.columns\n",
    "# promoters_df_final.columns\n",
    "# pricerange_df_broken_in_columns.columns\n",
    "# ticket_limit.columns\n",
    "# please_note.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bridal-destruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                            Comedian Gabriel \"Fluffy\" Iglesias will be performing at DPAC on November 4, 2021. This is part of his all new 2021 \"Beyond The Fluffy\" World Tour \"Go Big or Go Home\".\n",
       "1                                                                                                                                                                                                                                                                                                NaN\n",
       "2                                                              This event has been rescheduled to August 28th, 2021. Original tickets honored for the new date. Cody Ko and Noel Miller bring their Tiny Meat Gang Live - Global Domination Tour to DPAC in Durham, NC on Saturday, August 28, 2021.\n",
       "3                                                                                                                                                     This event has been rescheduled to August 5th, 2021 at 8:00pm A.R. Rahman, the man who has redefined contemporary Indian music, comes to DPAC.\n",
       "4    The six men comprising Styx have committed to rocking the Paradise together with audiences far and wide by entering their second decade of averaging over 100 shows a year, and each one of them is committed to making the next show better than the last including at DPAC on August 2, 2020.\n",
       "Name: info, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_1['info'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "homeless-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
